var documenterSearchIndex = {"docs":
[{"location":"defining_momdp/#Defining-a-MOMDP","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"Since MOMDPs are a factored POMDP, we can integrate with the existing POMDPs.jl framework. In particular, the MOMDP type extends the POMDP type where the state is a tuple of the fully observed state and the partially observed state.","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"For discrete problems where we explictly are defining the problem, instead of defining the transition, states, stateindex, and initialstate functions, you need to define them for the fully observable and hidden state spaces separately.","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"For all other functions (e.g. observation, reward, etc.), the functions are exported from POMDPs.jl and need to be defined using a state as a tuple (x::X, y::Y) where X and Y are the types of the fully observed and partially observed states, respectively.","category":"page"},{"location":"defining_momdp/#State-Space","page":"Defining a MOMDP","title":"State Space","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"To define the state space for a MOMDP, the following functions are required:","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"states_x(m::MOMDP): Returns the set of all fully observable states.\nstates_y(m::MOMDP): Returns the set of all partially observable states.","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"To index these states separately, use:","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"stateindex_x(m::MOMDP, state): Returns the index of the visible state x where state is a tuple of the form (x, y).\nstateindex_y(m::MOMDP, state): Returns the index of the hidden state y where state is a tuple of the form (x, y).","category":"page"},{"location":"defining_momdp/#Initial-State-Distribution","page":"Defining a MOMDP","title":"Initial State Distribution","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"The initial state distribution describes the probabilities of beginning the MOMDP in particular states. Given the factorized nature of MOMDPs, these distributions can also be factorized into two components:","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"initialstate_x(m::MOMDP): Provides the probability distribution over the fully observable states at the initial timestep.\ninitialstate_y(m::MOMDP, x): Given a fully observable initial state x, this function returns the initial probability distribution over the partially observable states.","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"Additionally, a helper function, is_initial_distribution_independent(::MOMDP), can be used to indicate if the initial partially observable state distribution is independent of the initial fully observable state. Returning true here signals that the partially observable state distribution is unaffected by the choice of initial observable state. This helps speed up some parts of solvers (e.g. writing the initial state distribution when constructing the pomdpx file for SARSOP).","category":"page"},{"location":"defining_momdp/#Transition-Functions","page":"Defining a MOMDP","title":"Transition Functions","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"Transitions are separately defined for the observable and hidden parts of the state:","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"transition_x(m::MOMDP, state, action): Provides the distribution over the next fully observable state x^prime given the current state (x y) and action a.\ntransition_y(m::MOMDP, state, action, statep_visible): Returns the probability distribution over the next partially observable state y^prime given the current state (x y), action a, and the resulting fully observable state x^prime.","category":"page"},{"location":"defining_momdp/#Conditional-Dependencies","page":"Defining a MOMDP","title":"Conditional Dependencies","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"As a default, MOMDPs.jl assumes the structure as shown in the diagram. However, a lot of problems have a simpler structure. For example, the visible state x^prime could only depend on the previous visible state x and action a. ","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"<img src=\"../images/momdp_diagram_color.svg\" alt=\"MOMDP Diagram Highlighted\" style=\"width: 40%;\">","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"The prescence of the colored edges can designated using the following functions:","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"Edge label (1): is_y_prime_dependent_on_x_prime(m::MOMDP)\nEdge label (2): is_x_prime_dependent_on_y(m::MOMDP)","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"As a default, these functions return true, indicating the presence of the edges and thus the standard MOMDP structure. However, it is highly recommended to set these functions appropriately for your problem in order to improve performance of solvers.","category":"page"},{"location":"defining_momdp/#Helper-Functions","page":"Defining a MOMDP","title":"Helper Functions","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"While states, stateindex, and initialstate are not required to be defined for a MOMDP, there are situations where having these function defined are useful. For discrete MODMPs, we extend these functions using the defined MOMDP functions.","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"states(m::MOMDP): Returns the full state space as a vector of Tuple{X,Y} where X is the type of the fully observable state and Y is the type of the partially observable state. This functions uses states_x and states_y to construct the full state space.\nstateindex(m::MOMDP, s): Returns the index of the state s in the state space. This function uses stateindex_x and stateindex_y.\ntransition(m::MOMDP, s, a): Returns the transition distribution over the next state (each state is a tuple of (x, y)) given the current state and action. This function uses transition_x and transition_y to construct the full transition distribution.\ninitialstate(m::MOMDP): Returns the initial state distribution over the full state space. This function uses initialstate_x and initialstate_y to construct the full initial state distribution.","category":"page"},{"location":"defining_momdp/#POMDP_of_Discrete_MOMDP","page":"Defining a MOMDP","title":"POMDP_of_Discrete_MOMDP","text":"","category":"section"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"The POMDP_of_Discrete_MOMDP type is a wrapper around a MOMDP that allows for the use of existing POMDP solvers. It is a subtype of POMDP and thus can be used with any POMDP solver that is compatible with POMDPs.jl.","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"This type is used to convert a Discrete MOMDP into a POMDP. There are functions defined for this POMDP that use the functions defined for the MOMDP type. Primarily, it uses the helper functions defined above. ","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"The resulting state space is the Cartesian product of the fully observable state space and the partially observable state space mathcalX times mathcalY. This might result in a slightly larger state space than required (e.g. if you define a single terminal state in either the fully observable or partially observable factored state spaces).","category":"page"},{"location":"defining_momdp/","page":"Defining a MOMDP","title":"Defining a MOMDP","text":"If the original observation space of the MOMDP is of size mathcalO and the visible state space is of size mathcalX, then the observation space of the POMDP_of_Discrete_MOMDP is mathcalX times mathcalO. Each observation is a tuple of the form (x, o) where x is a fully observable state and o is an observation.","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Exported-Types","page":"API","title":"Exported Types","text":"","category":"section"},{"location":"api/#MOMDPs.MOMDP","page":"API","title":"MOMDPs.MOMDP","text":"MOMDP{X, Y, A, O} <: POMDP{Tuple{X,Y},A,O}\n\nAbstract base type for a mixed observable Markov decision process.\n\nX: visible state type\nY: hidden state type\nA: action type\nO: observation type\n\nNotation matching Ong, Sylvie CW, et al. \"POMDPs for robotic tasks with mixed observability.\" Robotics: Science and systems. Vol. 5. No. 4. 2009. link\n\n\n\n\n\n","category":"type"},{"location":"api/#MOMDPs.POMDP_of_Discrete_MOMDP","page":"API","title":"MOMDPs.POMDP_of_Discrete_MOMDP","text":"POMDP_of_Discrete_MOMDP{X,Y,A,O} <: POMDP{Tuple{X,Y},A,O}\n\nThis type is used to convert a Discrete MOMDP into a POMDP. There are functions defined for this POMDP that use the functions defined for the MOMDP type. \n\nThe only difference in the spaces is the observation space. If the original observation space was of size mathcalO and the visible state space was of size mathcalX, then the observation space of the POMDP is mathcalX times mathcalO.\n\n\n\n\n\n","category":"type"},{"location":"api/#MOMDPs.MOMDPAlphaVectorPolicy","page":"API","title":"MOMDPs.MOMDPAlphaVectorPolicy","text":"MOMDPAlphaVectorPolicy(momdp::MOMDP, alpha_vecs, action_map, vis_state_map)\n\nConstruct a policy from alpha vectors for a Mixed Observability Markov Decision Process (MOMDP).\n\nArguments\n\nmomdp::MOMDP: The MOMDP problem instance for which the policy is constructed.\nalpha_vecs: An abstract vector of alpha vectors, where each alpha vector is a vector of floats representing the value function for a particular belief state.\naction_map: A vector of actions corresponding to each alpha vector. Each action is associated with the alpha vector that prescribes it.\nvis_state_map: A vector mapping visible states to their corresponding alpha vectors, used to determine which alpha vector applies to a given state.\n\nFields\n\nmomdp::MOMDP: The MOMDP problem instance, necessary for mapping states to locations in the alpha vectors.\nn_states_x::Int: The number of visible states in the MOMDP.\nn_states_y::Int: The number of hidden states in the MOMDP.\nalphas::Vector{Vector{Vector{Float64}}}: A vector (of size mathcalX) of vectors (number of alpha vectors) of vectors (number of alpha vectors) of alpha vectors (of size mathcalY). This structure holds the alpha vectors for each visible state.\naction_map::Vector{Vector{A}}: A vector (of size |X|) of vectors of actions corresponding to the alpha vectors. Each action is associated with a specific alpha vector for a visible state.\n\nThis structure represents a policy that uses alpha vectors to determine the best action to take given a belief state in a MOMDP.\n\n\n\n\n\n","category":"type"},{"location":"api/#MOMDPs.MOMDPDiscreteUpdater","page":"API","title":"MOMDPs.MOMDPDiscreteUpdater","text":"MOMDPDiscreteUpdater\n\nAn updater type for maintaining and updating discrete beliefs over hidden states in a Mixed Observability Markov Decision Process (MOMDP).\n\nConstructor\n\nMOMDPDiscreteUpdater(momdp::MOMDP)\n\nCreate a discrete belief updater for the given MOMDP.\n\nFields\n\nmomdp <: MOMDP: The MOMDP problem instance for which beliefs will be updated\n\nDescription\n\nIn a MOMDP, the state space is factored into visible states x (fully observable) and  hidden states y (partially observable). This updater maintains beliefs only over the  hidden states, since the visible states are assumed to be directly observed.\n\nThe updater implements the discrete Bayesian filter for belief updates, assuming:\n\nFinite, discrete hidden state spaces\nKnown visible state transitions (x → x')\nProbabilistic hidden state transitions that may depend on visible states\nObservations that depend on both visible and hidden states\n\nUsage\n\nmomdp = YourMOMDPProblem()\nupdater = MOMDPDiscreteUpdater(momdp)\n\n# Initialize belief over hidden states\ninitial_dist = initialstate_y(momdp)\n\n# Update belief after taking action and receiving observation\nnew_belief = update(updater, current_belief, action, observation, x, xp)\n\nSee Also\n\nuniform_belief_y: Create uniform beliefs over hidden states\ninitialize_belief: Initialize beliefs from distributions\nupdate: Perform belief updates using the MOMDP filter\n\n\n\n\n\n","category":"type"},{"location":"api/#Exported-Functions","page":"API","title":"Exported Functions","text":"","category":"section"},{"location":"api/#MOMDPs.transition_x","page":"API","title":"MOMDPs.transition_x","text":"transition_x(m::MOMDP{X,Y,A,O}, state::Tuple{X,Y}, action)\n\nReturn the transition distribution over the next visible state given the current state and action.\n\nT_x(s, a, x′) = p(x′ | s, a) where s = (x,y)\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.transition_y","page":"API","title":"MOMDPs.transition_y","text":"transition_y(m::MOMDP{X,Y,A,O}, state::Tuple{X,Y}, action, statep_visible)\n\nReturn the transition distribution over the next hidden state given the current state, action, and next visible state.\n\nT_y(s, a, x′, y′) = p(y′ | s, a, x′) where s = (x,y)\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.states_x","page":"API","title":"MOMDPs.states_x","text":"states_x(problem::MOMDP)\n\nReturns the complete visible state space of a MOMDP.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.states_y","page":"API","title":"MOMDPs.states_y","text":"states_y(problem::MOMDP)\n\nReturns the complete hidden state space of a MOMDP.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.stateindex_x","page":"API","title":"MOMDPs.stateindex_x","text":"stateindex_x(problem::MOMDP, s)\n\nReturn the integer index of the visible state x where s is a tuple of the form (x,y). Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.stateindex_y","page":"API","title":"MOMDPs.stateindex_y","text":"stateindex_y(problem::MOMDP, s)\n\nReturn the integer index of the hidden state y where s is a tuple of the form (x,y). Used for discrete models only.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.initialstate_x","page":"API","title":"MOMDPs.initialstate_x","text":"initialstate_x(problem::MOMDP)\n\nReturn the initial visible state distribution.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.initialstate_y","page":"API","title":"MOMDPs.initialstate_y","text":"initialstate_y(problem::MOMDP, x)\n\nReturn the initial hidden state distribution conditioned on the visible state x.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.statetype_x","page":"API","title":"MOMDPs.statetype_x","text":"statetype_x(t::Type)\nstatetype_x(p::MOMDP)\n\nReturn the visible state type for a MOMDP (the X in MOMDP{X,Y,A,O}).\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.statetype_y","page":"API","title":"MOMDPs.statetype_y","text":"statetype_y(t::Type)\nstatetype_y(p::MOMDP)\n\nReturn the hidden state type for a MOMDP (the Y in MOMDP{X,Y,A,O}).\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.ordered_states_x","page":"API","title":"MOMDPs.ordered_states_x","text":"ordered_states_x(momdp)\n\nReturn an AbstractVector of the visible states in a MOMDP ordered according to stateindex_x(momdp, s).\n\nordered_states_x(momdp) will always return a AbstractVector{X} v containing all of the visible states in states_x(momdp) in the order such that stateindex_x(momdp, v[i]) == i. You may wish to override this for your problem for efficiency.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.ordered_states_y","page":"API","title":"MOMDPs.ordered_states_y","text":"ordered_states_y(momdp)\n\nReturn an AbstractVector of the hidden states in a MOMDP ordered according to stateindex_y(momdp, s).\n\nordered_states_y(momdp) will always return a AbstractVector{Y} v containing all of the hidden states in states_y(momdp) in the order such that stateindex_y(momdp, v[i]) == i. You may wish to override this for your problem for efficiency.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.is_y_prime_dependent_on_x_prime","page":"API","title":"MOMDPs.is_y_prime_dependent_on_x_prime","text":"is_y_prime_dependent_on_x_prime(m::MOMDP)\n\nDefines if the next hidden state y′ depends on the next visible state x′ given the current visible state x, hidden state y, and action a.\n\nReturn false if the conditional probability distribution satisfies: p(y′ | x, y, a, x′) = p(y′ | x, y, a).\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.is_x_prime_dependent_on_y","page":"API","title":"MOMDPs.is_x_prime_dependent_on_y","text":"is_x_prime_dependent_on_y(m::MOMDP)\n\nDefines if the next visible state x′ depends on the current hidden state y given the current visible state x and action a.\n\nReturns false if the conditional probability distribution satisfies: p(x′ | x, y, a) = p(x′ | x, a).\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.is_initial_distribution_independent","page":"API","title":"MOMDPs.is_initial_distribution_independent","text":"is_initial_distribution_independent(m::MOMDP)\n\nDefines whether the initial distributions of the visible state x and hidden state y are independent.\n\nReturns true if the joint probability distribution satisfies: p(x, y) = p(x)p(y), meaning x and y are independent in the initial distribution.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.beliefvec_y","page":"API","title":"MOMDPs.beliefvec_y","text":"beliefvec_y(m::MOMDP, n_states::Int, b)\n\nConvert a belief distribution b over hidden states to a vector representation suitable  for dot product operations with alpha vectors in a MOMDP.\n\nArguments\n\nm::MOMDP: The MOMDP problem instance\nn_states_y::Int: The number of hidden states in the MOMDP\nb: The belief distribution over hidden states (supports various belief types)\n\nReturns\n\nA vector of length n_states_y where element i represents the probability of hidden state i\n\nSupported Belief Types\n\nSparseCat: Converts sparse categorical distribution to dense vector\nAbstractVector: Returns the vector directly (with length assertion)\nDiscreteBelief: Extracts the underlying probability vector\nDeterministic: Creates a one-hot vector for the deterministic state\n\nThis function is used internally by alpha vector policies to convert belief distributions into the vector format required for computing dot products with alpha vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.uniform_belief_y","page":"API","title":"MOMDPs.uniform_belief_y","text":" uniform_belief_y(momdp)\n uniform_belief_y(up::MOMDPDiscreteUpdater)\n\nReturn a uniform DiscreteBelief over all hidden states in the MOMDP.\n\nArguments\n\nmomdp: A MOMDP problem instance, or\nup::MOMDPDiscreteUpdater: A MOMDP discrete belief updater\n\nReturns\n\nA DiscreteBelief with equal probability 1/|Y| for each hidden state y ∈ Y\n\nDescription\n\nThis function creates a uniform prior belief over the hidden state space, which is often  used as an initial belief when the true hidden state is unknown. In MOMDP problems, this  represents maximum uncertainty about the hidden state while the visible state is assumed  to be known.\n\nThe uniform belief is particularly useful for:\n\nInitializing belief when no prior information is available\nBaseline comparisons in experiments\nWorst-case analysis of belief-dependent policies\n\nExample\n\nmomdp = YourMOMDPProblem()\ninitial_belief = uniform_belief_y(momdp)\n# or\nupdater = MOMDPDiscreteUpdater(momdp)\ninitial_belief = uniform_belief_y(updater)\n\n\n\n\n\n","category":"function"},{"location":"api/#Extended-Functions","page":"API","title":"Extended Functions","text":"","category":"section"},{"location":"api/#POMDPs.jl-and-POMDPTools.jl","page":"API","title":"POMDPs.jl and POMDPTools.jl","text":"","category":"section"},{"location":"api/#POMDPs.states-Tuple{MOMDP}","page":"API","title":"POMDPs.states","text":"POMDPs.states(p::MOMDP)\n\nHelper function to return the full state space for discrete MOMDPs. The states are     Tuple{X,Y} where X is the visible state and Y is the hidden state.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.stateindex-Union{Tuple{O}, Tuple{A}, Tuple{Y}, Tuple{X}, Tuple{MOMDP{X, Y, A, O}, Tuple{X, Y}}} where {X, Y, A, O}","page":"API","title":"POMDPs.stateindex","text":"POMDPs.stateindex(p::MOMDP{X,Y,A,O}, s::Tuple{X,Y}) where {X,Y,A,O}\n\nHelper function to return the index of the Tuple{X,Y} state for discrete MOMDPs.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.transition-Union{Tuple{O}, Tuple{A}, Tuple{Y}, Tuple{X}, Tuple{MOMDP{X, Y, A, O}, Tuple{X, Y}, A}} where {X, Y, A, O}","page":"API","title":"POMDPs.transition","text":"POMDPs.transition(p::MOMDP{X,Y,A,O}, s::Tuple{X,Y}, a::A) where {X,Y,A,O}\n\nHelper function to return the full transition distribution for discrete MOMDPs. The states      are Tuple{X,Y}. It uses transition_x and transition_y to construct the     distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.initialstate-Union{Tuple{MOMDP{X, Y, A, O}}, Tuple{O}, Tuple{A}, Tuple{Y}, Tuple{X}} where {X, Y, A, O}","page":"API","title":"POMDPs.initialstate","text":"initialstate(p::MOMDP{X,Y,A,O}) where {X,Y,A,O}\n\nHelper function to return the initial state distribution for discrete MOMDPs. The states      are Tuple{X,Y}. It uses initialstate_x and initialstate_y to construct the     distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.observations-Tuple{POMDP_of_Discrete_MOMDP}","page":"API","title":"POMDPs.observations","text":"POMDPs.observations(p::POMDP_of_Discrete_MOMDP)\n\nReturns the full observation space of a POMDPofDiscrete_MOMDP. The observations are Tuple{X,O} where X is the visible state and O is the observation.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.observation-Union{Tuple{O}, Tuple{A}, Tuple{Y}, Tuple{X}, Tuple{POMDP_of_Discrete_MOMDP{X, Y, A, O}, A, Tuple{X, Y}}} where {X, Y, A, O}","page":"API","title":"POMDPs.observation","text":"POMDPs.observation(p::POMDP_of_Discrete_MOMDP{X, Y, A, O}, a::A, s::Tuple{X, Y}) where {X, Y, A, O}\n\nReturns the full observation distribution for a POMDPofDiscrete_MOMDP. The observations are Tuple{X,O} where X is the visible state and O is the observation.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.obsindex-Tuple{POMDP_of_Discrete_MOMDP, Any}","page":"API","title":"POMDPs.obsindex","text":"POMDPs.obsindex(p::POMDP_of_Discrete_MOMDP, o)\n\nReturns the index of the Tuple{X,O} observation for a POMDPofDiscrete_MOMDP.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.value-Tuple{MOMDPAlphaVectorPolicy, Any}","page":"API","title":"POMDPs.value","text":"value(p::MOMDPAlphaVectorPolicy, b)\n\nCalculate the value of belief state b for a MOMDP using alpha vectors.\n\nThis function computes the value by:\n\nMarginalizing the belief over the partially observable state to get b(x) for each fully observable state x\nComputing the conditional belief b(y mid x) = b(xy)b(x) for each x\nFinding the maximum dot product between b(y mid x) and the alpha vectors for each x\nSumming b(x) cdot V(x b(y mid x)) over all x to get the total value\n\nArguments\n\np::MOMDPAlphaVectorPolicy: The alpha vector policy\nb: The belief state over the joint state space (x,y)\n\nReturns\n\nThe value of the belief state\n\nNotes\n\nThis is not the most efficient way to get a value if we are operating in a true MOMDP framework. However, this keeps the structure of the code similar to the POMDPs.jl framework.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.value-Tuple{MOMDPAlphaVectorPolicy, Any, Any}","page":"API","title":"POMDPs.value","text":"value(p::MOMDPAlphaVectorPolicy, b, x)\n\nCalculate the value for a specific visible state x given a belief b over the joint state space.\n\nArguments\n\np::MOMDPAlphaVectorPolicy: The alpha vector policy\nb: The belief distribution over the joint state space (x,y)\nx: The specific visible state to evaluate\n\nReturns\n\nThe value for the given visible state and belief\n\nDescription\n\nThis function computes the value by:\n\nExtracting the marginal belief over hidden states y for the given visible state x by evaluating pdf(b, (x,y)) for all y\nFinding the maximum dot product between this marginal belief and all alpha vectors associated with visible state x\n\nThis is more efficient than the general value(p, b) function when the visible state is known, as it only needs to consider alpha vectors for the specific x rather than marginalizing over all visible states.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.action-Tuple{MOMDPAlphaVectorPolicy, Any}","page":"API","title":"POMDPs.action","text":"action(p::MOMDPAlphaVectorPolicy, b)\n\nReturn the action prescribed by the MOMDP alpha-vector policy p for the belief b over the joint state space (x, y).\n\nHeuristic\n\nFind the visible state x with the largest probability mass in b.\nForm the conditional distribution over y given that x.\nAmong the alpha-vectors in p.alphas[x], pick the one with the largest dot product  with that conditional distribution.\nReturn the action associated with that alpha-vector.\n\nNotes\n\nWhen b is not a pure distribution over a single x: Typically in a MOMDP,  we assume x (the \"visible\" state) is fully observed at runtime, so the  belief over (x y) will place essentially all its probability mass on a  single x. In that case, the above steps are effectively picking the single  x that we actually observed.\nIf you are operating in a true MOMDP framework, you can implement a custom action function that takes in the visible state x and the conditional distribution over y given x. While this would result in the same action as the heuristic above, it will be more efficient. E.g.: \n# If ``x`` is known exactly at runtime and we have a distribution only over ``y``:\nfunction action(p::MOMDPAlphaVectorPolicy, x, by)\n    x_idx = stateindex_x(p.momdp, x)\n    # pick the alpha-vector among p.alphas[x_idx] that maximizes dot(alpha, by)\n    ...\nend\nIn case your solver or simulator still gives a multi-modal distribution  over different x (which can happen in generic POMDP frameworks), the code  here picks the x that has the largest total probability mass in the belief.  While this heuristic might be sufficient for some problems, we recommend implementing a custom action function that performs a one-step lookahead using the value function.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.action-Tuple{MOMDPAlphaVectorPolicy, Any, Any}","page":"API","title":"POMDPs.action","text":"action(p::MOMDPAlphaVectorPolicy, b, x)\n\nReturn the action prescribed by the MOMDP alpha-vector policy p for the belief b over the hidden states, given the known visible state x.\n\nArguments\n\np::MOMDPAlphaVectorPolicy: The alpha vector policy\nb: The belief distribution over hidden states y\nx: The known visible state\n\nReturns\n\nThe action for the given visible state and belief over hidden states\n\nDescription\n\nThis function assumes the visible state x is fully observed and known. It:\n\nConverts the belief b to a vector representation over hidden states\nAmong all alpha vectors associated with visible state x, finds the one with  the maximum dot product with the belief vector\nReturns the action associated with that optimal alpha vector\n\nThis is more efficient than the version without explicit x when the visible state is known at runtime, as it avoids the need to infer x from the belief distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPTools.Policies.actionvalues-Tuple{MOMDPAlphaVectorPolicy, Any, Any}","page":"API","title":"POMDPTools.Policies.actionvalues","text":"actionvalues(p::MOMDPAlphaVectorPolicy, b, x)\n\nCompute the action values (Q-values) for all actions given a belief b over hidden  states and a known visible state x.\n\nArguments\n\np::MOMDPAlphaVectorPolicy: The alpha vector policy\nb: The belief distribution over hidden states y  \nx: The known visible state\n\nReturns\n\nA vector of action values where the ith element is the Q-value for action i\n\nDescription\n\nThis function performs a one-step lookahead to compute action values by:\n\nFor each action a and each hidden state y in the belief support:\nComputing the immediate reward R(x,y,a)\nFor each possible next visible state x' and hidden state y':\nFor each possible observation o:\nUpdating the belief to get b'\nComputing the value using the alpha vectors for x'\nAccumulating the discounted expected future value\nSumming over all transitions weighted by their probabilities\n\nThe resulting Q-values can be used for action selection or policy evaluation when the visible state is known.\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.initialize_belief-Tuple{MOMDPDiscreteUpdater, Any}","page":"API","title":"POMDPs.initialize_belief","text":"initialize_belief(bu::MOMDPDiscreteUpdater, dist::Any)\n\nInitialize a discrete belief over hidden states from a given distribution.\n\nArguments\n\nbu::MOMDPDiscreteUpdater: The MOMDP discrete belief updater\ndist: A distribution over hidden states to initialize from (supports various distribution types)\n\nReturns\n\nA DiscreteBelief over the hidden state space with probabilities initialized from dist\n\nDescription\n\nThis function creates a discrete belief representation over the hidden states y suitable  for use with MOMDP belief update operations. The conversion process:\n\nCreates a zero-initialized probability vector over all hidden states in the MOMDP\nFor each state y in the support of the input distribution, extracts the probability  pdf(dist, y) and assigns it to the corresponding index in the belief vector\nReturns a DiscreteBelief object that can be used with the MOMDP updater\n\nSupported Distribution Types\n\nThe function can handle various distribution types through the generic pdf interface:\n\nDiscrete distributions (e.g., Categorical, DiscreteUniform)\nCustom distributions that implement pdf and support\nSparse distributions with limited support\n\nUsage Examples\n\nupdater = MOMDPDiscreteUpdater(momdp)\n\n# From a uniform distribution\nuniform_dist = DiscreteUniform(1, length(states_y(momdp)))\nbelief = initialize_belief(updater, uniform_dist)\n\n# From a sparse categorical distribution  \nsparse_dist = SparseCat([state1, state3], [0.7, 0.3])\nbelief = initialize_belief(updater, sparse_dist)\n\nImplementation Notes\n\nUses stateindex_y to map hidden states to belief vector indices\nAssumes the distribution is over individual hidden states, not joint (x,y) states\nThe resulting belief is properly normalized if the input distribution is normalized\n\n\n\n\n\n","category":"method"},{"location":"api/#POMDPs.update-Tuple{MOMDPDiscreteUpdater, Vararg{Any, 5}}","page":"API","title":"POMDPs.update","text":"update(bu::MOMDPDiscreteUpdater, b::Any, a, o, x, xp)\n\nUpdate a belief of any type by first converting it to a discrete belief, then updating.\n\nArguments\n\nbu::MOMDPDiscreteUpdater: The MOMDP discrete belief updater\nb::Any: The current belief (will be converted to discrete belief if needed)\na: The action taken\no: The observation received after taking action a\nx: The previous visible state\nxp: The current visible state\n\nReturns\n\nA new DiscreteBelief representing the updated belief over hidden states\n\nDescription\n\nThis is a convenience method that handles arbitrary belief types by first calling initialize_belief to convert them to a DiscreteBelief, then performing the standard discrete belief update.\n\n\n\n\n\n","category":"method"},{"location":"api/#Internal-Functions","page":"API","title":"Internal Functions","text":"","category":"section"},{"location":"api/#MOMDPs.alphapairs","page":"API","title":"MOMDPs.alphapairs","text":"Return an iterator of alpha vector-action pairs in the policy, given a visible state.\n\n\n\n\n\n","category":"function"},{"location":"api/#MOMDPs.alphavectors","page":"API","title":"MOMDPs.alphavectors","text":"Return the alpha vectors, given a visible state.\n\n\n\n\n\n","category":"function"},{"location":"belief_updaters/#Belief-Updaters","page":"Belief Updaters","title":"Belief Updaters","text":"","category":"section"},{"location":"belief_updaters/","page":"Belief Updaters","title":"Belief Updaters","text":"The only updater currently implemented is the MOMDPDiscreteUpdater.","category":"page"},{"location":"belief_updaters/#MOMDPDiscreteUpdater","page":"Belief Updaters","title":"MOMDPDiscreteUpdater","text":"","category":"section"},{"location":"belief_updaters/","page":"Belief Updaters","title":"Belief Updaters","text":"The MOMDPDiscreteUpdater is a discrete belief updater for MOMDPs. The update function implements the discrete Bayesian filter for MOMDPs, which updates beliefs over hidden states given knowledge of visible state transitions. The update function requires the current belief b, the action taken a, the observation received o, the visible state from which the action was taken x, and the visible state that we transitioned to xp.","category":"page"},{"location":"belief_updaters/#Example-Usage","page":"Belief Updaters","title":"Example Usage","text":"","category":"section"},{"location":"belief_updaters/","page":"Belief Updaters","title":"Belief Updaters","text":"momdp = YourMOMDPProblem()\nupdater = MOMDPDiscreteUpdater(momdp)\n\n# Initialize belief over hidden states\ninitial_dist = initialstate_y(momdp)\n\n# Update belief after taking action and receiving observation\nnew_belief = update(updater, current_belief, action_taken, observation_received, x, xp)","category":"page"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"The process of defining, solving, and evaluating a MOMDP closely mirrors the steps for a POMDP, differing primarily in the function definitions required.","category":"page"},{"location":"examples/#RockSample","page":"Examples","title":"RockSample","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"We will use the classic Rock Sample problem to demonstrate forming a MOMDP, solving it with SARSOP, and evaluating the policy. Since RockSample is already defined as a POMDP in RockSample.jl, we will reuse existing definitions where possible and focus on the MOMDP-specific aspects.","category":"page"},{"location":"examples/#RockSampleMOMDP-Type","page":"Examples","title":"RockSampleMOMDP Type","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"We define the MOMDP type similarly to the existing POMDP and provide a constructor from the POMDP type.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using POMDPs\nusing POMDPTools\nusing MOMDPs\nusing Printf \nusing LinearAlgebra\nusing RockSample\nusing StaticArrays # for SVector\n\nmutable struct RockSampleMOMDP{K} <: MOMDP{RSPos,SVector{K,Bool},Int,Int}\n    map_size::Tuple{Int,Int}\n    rocks_positions::SVector{K,RSPos}\n    init_pos::RSPos\n    sensor_efficiency::Float64\n    bad_rock_penalty::Float64\n    good_rock_reward::Float64\n    step_penalty::Float64\n    sensor_use_penalty::Float64\n    exit_reward::Float64\n    terminal_state::RSPos\n    discount_factor::Float64\nend\n\n\"\"\"\n    RockSampleMOMDP(rocksample_pomdp::RockSamplePOMDP)\n\nCreate a RockSampleMOMDP using the same parameters in a RockSamplePOMDP.\n\"\"\"\nfunction RockSampleMOMDP(rocksample_pomdp::RockSamplePOMDP)\n    return RockSampleMOMDP(\n        rocksample_pomdp.map_size,\n        rocksample_pomdp.rocks_positions,\n        rocksample_pomdp.init_pos,\n        rocksample_pomdp.sensor_efficiency,\n        rocksample_pomdp.bad_rock_penalty,\n        rocksample_pomdp.good_rock_reward,\n        rocksample_pomdp.step_penalty,\n        rocksample_pomdp.sensor_use_penalty,\n        rocksample_pomdp.exit_reward,\n        rocksample_pomdp.terminal_state.pos,\n        rocksample_pomdp.discount_factor\n    )\nend","category":"page"},{"location":"examples/#State-Space","page":"Examples","title":"State Space","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"In RockSample, the robot knows its location but observes only rock states. Thus, grid locations form the visible state and rock states form the hidden state.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Visible states: All possible grid locations and a terminal state\nfunction MOMDPs.states_x(problem::RockSampleMOMDP)\n    map_states = vec([SVector{2,Int}((i, j)) for i in 1:problem.map_size[1], j in 1:problem.map_size[2]])\n    push!(map_states, problem.terminal_state) # Add terminal state\n    return map_states\nend\n\n# Hidden states: All possible K-length vector of booleans, where K is the number of rocks\nfunction MOMDPs.states_y(problem::RockSampleMOMDP{K}) where {K}\n    bool_options = [[true, false] for _ in 1:K]\n    vec_bool_options = vec(collect(Iterators.product(bool_options...)))\n    s_vec_bool_options = [SVector{K,Bool}(bool_vec) for bool_vec in vec_bool_options]\n    return s_vec_bool_options\nend","category":"page"},{"location":"examples/#State-Indexing","page":"Examples","title":"State Indexing","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"For certain solvers, we also need the stateindex function defined. For MOMDPs, we need to define it for both the visible and hidden states.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function MOMDPs.stateindex_x(problem::RockSampleMOMDP, s::Tuple{RSPos, SVector{K,Bool}}) where {K}\n    return stateindex_x(problem, s[1])\nend\nfunction MOMDPs.stateindex_x(problem::RockSampleMOMDP, x::RSPos)\n    if isterminal(problem, (x, first(states_y(problem))))\n        return length(states_x(problem))\n    end\n    return LinearIndices(problem.map_size)[x[1], x[2]]\nend\n\nfunction MOMDPs.stateindex_y(problem::RockSampleMOMDP, s::Tuple{RSPos, SVector{K,Bool}}) where {K}\n    return stateindex_y(problem, s[2])\nend\nfunction MOMDPs.stateindex_y(problem::RockSampleMOMDP, y::SVector{K,Bool}) where {K}\n    return findfirst(==(y), states_y(problem))\nend","category":"page"},{"location":"examples/#Initial-State-Distributions","page":"Examples","title":"Initial State Distributions","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Similarly, we need to define the initial distribution over both the visible and hidden states. We can start with any distribution over the visible states, but in RockSample, we start at the init_pos defined in the problem.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function MOMDPs.initialstate_x(problem::RockSampleMOMDP)\n    return Deterministic(problem.init_pos)\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The distribution over hidden states is conditioned on the visible state. Therefore, initialstate_y has x as an input argument.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function MOMDPs.initialstate_y(::RockSampleMOMDP{K}, x::RSPos) where K\n    probs = normalize!(ones(2^K), 1)\n    states = Vector{SVector{K,Bool}}(undef, 2^K)\n    for (i,rocks) in enumerate(Iterators.product(ntuple(x->[false, true], K)...))\n        states[i] = SVector(rocks)\n    end\n    return SparseCat(states, probs)\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Notice that we didn't use x in the function initialstate_y. In RockSample, the initial distribution over the rock states is independent of the robot position. Therefore, we can set is_initial_distribution_independent to true.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"MOMDPs.is_initial_distribution_independent(::RockSampleMOMDP) = true","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"note: Note\nIf we plan on using the POMDPs.jl ecosystem, we still need to define initialstate(p::MOMDP). However, since our problem is discrete, we can use the initialstate(p::MOMDP) function defined in discrete_momdp_functions.jl using initialstate_x and initialstate_y.","category":"page"},{"location":"examples/#Action-Space","page":"Examples","title":"Action Space","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"There is no change in our action space from the POMDP version.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"POMDPs.actions(::RockSampleMOMDP{K}) where {K} = 1:RockSample.N_BASIC_ACTIONS+K\nPOMDPs.actionindex(::RockSampleMOMDP, a::Int) = a","category":"page"},{"location":"examples/#Transition-Funtions","page":"Examples","title":"Transition Funtions","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"For the transition function, we need to define both transition_x and transition_y. As a reminder, transition_x returns the distribution over the next visible state given the current state and action where the current state is defined as the tuple (x,y).","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For RockSample, it is a deterministic transition based on the action selected and the current visible state. We will use a similar helper function next_position as in the POMDP version.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function next_position(s::RSPos, a::Int)\n    if a > RockSample.N_BASIC_ACTIONS || a == 1\n        # robot check rocks or samples\n        return s\n    elseif a <= RockSample.N_BASIC_ACTIONS\n        # the robot moves \n        return s + RockSample.ACTION_DIRS[a]\n    end\nend\n\nfunction MOMDPs.transition_x(problem::RockSampleMOMDP, s::Tuple{RSPos,SVector{K,Bool}}, a::Int) where {K}\n    x = s[1]\n    if isterminal(problem, s)\n        return Deterministic(problem.terminal_state)\n    end\n    new_pos = next_position(x, a)\n    if new_pos[1] > problem.map_size[1]\n        new_pos = problem.terminal_state\n    else\n        new_pos = RSPos(clamp(new_pos[1], 1, problem.map_size[1]),\n            clamp(new_pos[2], 1, problem.map_size[2]))\n    end\n    return Deterministic(new_pos)\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"As we stated before defining the function, x_prime is only dependent on x and the action. Therefore, we can set is_x_prime_dependent_on_y to false.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"MOMDPs.is_x_prime_dependent_on_y(::RockSampleMOMDP) = false","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"transition_y returns the distribution over the next hidden state given the current state, action, and next visible state. In RockSample, this transition is also deterministic.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function MOMDPs.transition_y(problem::RockSampleMOMDP, s::Tuple{RSPos,SVector{K,Bool}}, a::Int, x_prime::RSPos) where {K}\n    if isterminal(problem, s)\n        return Deterministic(s[2])\n    end\n\n    if a == RockSample.BASIC_ACTIONS_DICT[:sample] && in(s[1], problem.rocks_positions)\n        rock_ind = findfirst(isequal(s[1]), problem.rocks_positions)\n        new_rocks = MVector{K,Bool}(undef)\n        for r = 1:K\n            new_rocks[r] = r == rock_ind ? false : s[2][r]\n        end\n        new_rocks = SVector(new_rocks)\n        \n    else # We didn't sample, so states of rocks remain unchanged\n        new_rocks = s[2]\n    end\n    \n    return Deterministic(new_rocks)\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Norice in our transition_y for RockSample that we did not use x_prime in the function. Therefore, we know the distritbuion of y_prime is conditionally independent of x_prime given the current state and action. Thus we can set is_y_prime_dependent_on_x_prime to false.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"MOMDPs.is_y_prime_dependent_on_x_prime(::RockSampleMOMDP) = false","category":"page"},{"location":"examples/#Observation-Space","page":"Examples","title":"Observation Space","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"In RockSample, we started with a known initial position of our robot and then the robot transitions in the grid are deterministic. Therefore, the location is always known through belief updates without needing an observation. The only observations needed in the POMDP version and in the MOMDP version are the results on the sensor.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"POMDPs.observations(::RockSampleMOMDP) = 1:3\nPOMDPs.obsindex(::RockSampleMOMDP, o::Int) = o\n\nfunction POMDPs.observation(problem::RockSampleMOMDP, a::Int, s::Tuple{RSPos,SVector{K,Bool}}) where {K}\n    if a <= RockSample.N_BASIC_ACTIONS\n        # no obs\n        return SparseCat((1, 2, 3), (0.0, 0.0, 1.0))\n    else\n        rock_ind = a - RockSample.N_BASIC_ACTIONS\n        rock_pos = problem.rocks_positions[rock_ind]\n        dist = norm(rock_pos - s[1])\n        efficiency = 0.5 * (1.0 + exp(-dist * log(2) / problem.sensor_efficiency))\n        rock_state = s[2][rock_ind]\n        if rock_state\n            return SparseCat((1, 2, 3), (efficiency, 1.0 - efficiency, 0.0))\n        else\n            return SparseCat((1, 2, 3), (1.0 - efficiency, efficiency, 0.0))\n        end\n    end\nend","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"If we wanted to start from any position (and thus the initial distribution would be uniform over the grid), then for the POMDP version we would need to increase our observation space to include the position of the robot. Therefore our observation space size would increase by a factor of mathcalX - 1 (since we have a terminal state within mathcalX). However, for a MOMDP, we do not need those observations, and our observation space would remain as defined.","category":"page"},{"location":"examples/#Other-Functions","page":"Examples","title":"Other Functions","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"There are no other changes to defining a MOMDP vs the POMDP using the explicit interface. However, we still need to define the reward function, terminal function, and discount factor.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"function POMDPs.reward(problem::RockSampleMOMDP, s::Tuple{RSPos,SVector{K,Bool}}, a::Int) where {K}\n    r = problem.step_penalty\n    if next_position(s[1], a)[1] > problem.map_size[1]\n        r += problem.exit_reward\n        return r\n    end\n\n    if a == RockSample.BASIC_ACTIONS_DICT[:sample] && in(s[1], problem.rocks_positions) # sample \n        rock_ind = findfirst(isequal(s[1]), problem.rocks_positions) # slow ?\n        r += s[2][rock_ind] ? problem.good_rock_reward : problem.bad_rock_penalty\n    elseif a > RockSample.N_BASIC_ACTIONS # using senssor\n        r += problem.sensor_use_penalty\n    end\n    return r\nend\n\nfunction POMDPs.isterminal(problem::RockSampleMOMDP, s::Tuple{RSPos,SVector{K,Bool}}) where {K}\n    return s[1] == problem.terminal_state\nend\n\nPOMDPs.discount(problem::RockSampleMOMDP) = problem.discount_factor","category":"page"},{"location":"examples/#Solving-using-SARSOP","page":"Examples","title":"Solving using SARSOP","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now that we have defined our MOMDP, we can solve it with SARSOP. We will create a POMDP RockSample problem and then a MOMDP RockSample problem from the POMDP since our constructor was defined with the POMDP type. Since we have the POMDP, we will also solve the POMDP using SARSOP so we can compare the policies.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"note: Note\nSARSOP.jl and POMDPXFiles.jl have not been updated to work wtih MOMDPs.jl. We must include test/pomdpxfiles.jl and test/sarsop.jl until the packages are updated. This note will be removed and the examples will be updated when the packages are updated.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using SARSOP\nusing POMDPXFiles\nusing ProgressMeter\n\ninclude(\"../../test/sarsop.jl\")\ninclude(\"../../test/pomdpxfiles.jl\")\n","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Create a smaller RockSample problem\nrocksample_pomdp = RockSample.RockSamplePOMDP(\n    map_size=(3, 3),\n    rocks_positions=[(2, 2), (3, 3), (1, 2)],\n    init_pos=(1, 1),\n    sensor_efficiency=0.5\n)\nrocksample_momdp = RockSampleMOMDP(rocksample_pomdp)\n\n# Instantiate the solver\nsolver_pomdp = SARSOPSolver(; precision=1e-2, timeout=30, \n    pomdp_filename=\"test_rocksample_pomdp.pomdpx\", verbose=false)\nsolver_momdp = SARSOPSolver(; precision=1e-2, timeout=30, \n    pomdp_filename=\"test_rocksample_momdp.pomdpx\", verbose=false)\n  \n# Solve the POMDP and the MOMDP\npolicy_pomdp = solve(solver_pomdp, rocksample_pomdp)\npolicy_momdp = solve(solver_momdp, rocksample_momdp)\n\n# Evaluate the policies at the initial belief\nb0_pomdp = initialstate(rocksample_pomdp)\nb0_momdp = initialstate(rocksample_momdp)\n\nval_pomdp_b0 = value(policy_pomdp, b0_pomdp)\nval_momdp_b0 = value(policy_momdp, b0_momdp)\n\n@printf(\"Value of POMDP policy: %.4f\\n\", val_pomdp_b0)\n@printf(\"Value of MOMDP policy: %.4f\\n\", val_momdp_b0)\n\n# What is the action of the policies at the initial belief?\na_pomdp_b0 = action(policy_pomdp, b0_pomdp)\na_momdp_b0 = action(policy_momdp, b0_momdp)\n\n@printf(\"Action of POMDP policy: %d\\n\", a_pomdp_b0)\n@printf(\"Action of MOMDP policy: %d\\n\", a_momdp_b0)","category":"page"},{"location":"examples/#Converting-to-a-POMDP","page":"Examples","title":"Converting to a POMDP","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"If you have a problem defined as a MOMDP, you can convert it to an equivalent POMDP. If your problem is discrete, you can use the POMDP_of_Discrete_MOMDP type. ","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"rocksample_pomdp_from_momdp = POMDP_of_Discrete_MOMDP(rocksample_momdp)\n\nsolver_pomdp_from_momdp = SARSOPSolver(; precision=1e-2, timeout=30, \n    pomdp_filename=\"test_rocksample_momdp.pomdpx\", verbose=false)\n\npolicy_pomdp_from_momdp = solve(solver_pomdp_from_momdp, rocksample_pomdp_from_momdp)\n\nb0_pomdp_from_momdp = initialstate(rocksample_pomdp_from_momdp)\nval_pomdp_from_momdp_b0 = value(policy_pomdp_from_momdp, b0_pomdp_from_momdp)\n\n@printf(\"Value of POMDP generated from MOMDP: %.4f\\n\", val_pomdp_from_momdp_b0)","category":"page"},{"location":"discrete_momdp_functions/#Discrete-Helper-Functions","page":"Discrete Helper Functions","title":"Discrete Helper Functions","text":"","category":"section"},{"location":"discrete_momdp_functions/","page":"Discrete Helper Functions","title":"Discrete Helper Functions","text":"The MOMDPs.jl package provides helper functions for discrete MOMDPs. These functions use the required functions to define a MOMDP to return the full state space, state index, transition distribution, and initial state distribution.","category":"page"},{"location":"discrete_momdp_functions/#States","page":"Discrete Helper Functions","title":"States","text":"","category":"section"},{"location":"discrete_momdp_functions/","page":"Discrete Helper Functions","title":"Discrete Helper Functions","text":"The function states(p::MOMDP) returns the full state space for a discrete MOMDP where the states are tuples of the visible state and the hidden state.","category":"page"},{"location":"discrete_momdp_functions/#State-Index","page":"Discrete Helper Functions","title":"State Index","text":"","category":"section"},{"location":"discrete_momdp_functions/","page":"Discrete Helper Functions","title":"Discrete Helper Functions","text":"The function stateindex(p::MOMDP{X,Y,A,O}, s::Tuple{X,Y}) where {X,Y,A,O} returns the index of the Tuple{X,Y} state for a discrete MOMDP.","category":"page"},{"location":"discrete_momdp_functions/#Transition-Distribution","page":"Discrete Helper Functions","title":"Transition Distribution","text":"","category":"section"},{"location":"discrete_momdp_functions/","page":"Discrete Helper Functions","title":"Discrete Helper Functions","text":"The function transition(p::MOMDP{X,Y,A,O}, s::Tuple{X,Y}, a::A) where {X,Y,A,O} returns the full transition distribution for a discrete MOMDP. The states are Tuple{X,Y}. It uses transition_x and transition_y to construct the distribution.","category":"page"},{"location":"discrete_momdp_functions/#Initial-State-Distribution","page":"Discrete Helper Functions","title":"Initial State Distribution","text":"","category":"section"},{"location":"discrete_momdp_functions/","page":"Discrete Helper Functions","title":"Discrete Helper Functions","text":"The function initialstate(p::MOMDP) returns the initial state distribution for a discrete MOMDP. The states are Tuple{X,Y}. It uses initialstate_x and initialstate_y to construct the distribution.","category":"page"},{"location":"#MOMDPs.jl","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"","category":"section"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"MOMDPs.jl provides structures and tools for Mixed Observability Markov Decision Processes (MOMDPs) in Julia. It is built on top of the POMDPs.jl framework, extending its functionality to a factorized state space composed of a fully observed state and a partially observed state.","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"note: Note\nThis package is in an initial/early stage. Feedback and contributions are highly encouraged!","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"To integrate MOMDPs.jl with other packages in the JuliaPOMDP ecosystem, you can extend key interface functions within those packages (similar to integrations already implemented in SARSOP.jl, POMDPXFiles.jl, etc.). ","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"If you want to define your problem as a MOMDP and find your desired solver or tool is not yet integrated, you have a few options:","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"Extend the desired functionality in your desired solver or tool.\nIf your problem is discrete, you can use the POMDP_of_Discrete_MOMDP type as it converts a MOMDP to a POMDP.\nImplement your own conversion using the functions defined for your MOMDP, similar to the conversion implemented for POMDP_of_Discrete_MOMDP.","category":"page"},{"location":"#What-are-MOMDPs?","page":"MOMDPs.jl","title":"What are MOMDPs?","text":"","category":"section"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"A MOMDP is a variant of the Partially Observable Markov Decision Process with a factored state: parts of the state are fully observed, and parts are partially observed. This can lead to more efficient planning solutions for domains that exhibit such structure.","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"Throughout this package, we will use the notation from Ong, Sylvie CW, et al. \"POMDPs for robotic tasks with mixed observability.\" Robotics: Science and systems. Vol. 5. No. 4. 2009..","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"We assume the MOMDP is of the following form (same diagram as from Figure 1 in Ong et al. 2009):","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"<img src=\"images/momdp_diagram.svg\" alt=\"MOMDP Diagram\" style=\"width: 40%;\">","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"A MOMDP is specified by the tuple (mathcalX mathcalY mathcalA mathcalO T_x T_y O R γ), where","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"mathcalX is the set of fully observed states\nmathcalY is the set of partially observed states\nmathcalA is the set of actions\nmathcalO is the set of observations\nT_x(x y a x^prime) = p(x^prime mid x y a) is the probability of transitioning to state x^prime from the current state (x y) and performing action a.\nT_y(x y a x^prime y^prime) = p(y^prime mid x y a x^prime) is the probability of transitioning to state y^prime after performing action a in state (x y) and the fully observable state is x^prime.\nO = p(o mid x y a) is the probability of observing o given the current state (x y) and action a.\nR = r(x y a) is the reward for taking action a in state (x y).\ngamma is the discount factor.","category":"page"},{"location":"#Installation","page":"MOMDPs.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"Use ] to get to the package manager to add the package. ","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"julia> ]\npkg> add MOMDPs","category":"page"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"MOMDPs.jl is built on top of POMDPs.jl and extends various other packages in the JuliaPOMDP ecosystem. When using MOMDPs.jl, you will also need these other packages. As a default, we recommend including POMDPs and POMDPTools.","category":"page"},{"location":"#Documentation-Outline","page":"MOMDPs.jl","title":"Documentation Outline","text":"","category":"section"},{"location":"","page":"MOMDPs.jl","title":"MOMDPs.jl","text":"Pages = reduce(vcat, map(last, Main.page_order))\nDepth = 3","category":"page"},{"location":"policies/#Policies","page":"Policies","title":"Policies","text":"","category":"section"},{"location":"policies/","page":"Policies","title":"Policies","text":"The MOMDP type currently has only been developed to work with alpha vector policies. If other policy types are desired, please open an issue on the GitHub repository. Until the issue has been resolved, you can convert your MOMDP to a POMDP using the POMDP_of_Discrete_MOMDP type (or implement your own conversion).","category":"page"},{"location":"policies/#MOMDPAlphaVectorPolicy","page":"Policies","title":"MOMDPAlphaVectorPolicy","text":"","category":"section"},{"location":"policies/","page":"Policies","title":"Policies","text":"The MOMDPAlphaVectorPolicy type is similar to the AlphaVectorPolicy from POMDPTools.jl. The main difference is how the alpha vectors are stored. AlphaVectorPolicy stores alpha vectors as a vector of alpha vectors, i.e. Vector{Vector{Float64}}. ","category":"page"},{"location":"policies/","page":"Policies","title":"Policies","text":"A MOMDP value function V(x b_y) can be represented as a collection of vector sets Gamma_y(x) mid x in mathcalX. Therefore, in MOMDPAlphaVectorPolicy, we have a vector of alpha vectors for each visible state and the size of the alpha vector is mathcalY (the number of hidden states). Therefore, we have a vector of vectors of alpha vectors, i.e. Vector{Vector{Vector{Float64}}}.","category":"page"},{"location":"policies/","page":"Policies","title":"Policies","text":"For the action map (action_map), we also have a vector of size mathcalX (the number of visible states) that contains a vector of actions associated with each alpha vector.","category":"page"},{"location":"policies/#Value-Function","page":"Policies","title":"Value Function","text":"","category":"section"},{"location":"policies/","page":"Policies","title":"Policies","text":"With an alpha vector policy represented as MOMDPAlphaVectorPolicy (a collection of alpha vector sets), can use our visible state to determine the appropriate set and then find the maximum alpha vector in the set V(x b_mathcalY) = max_alpha in Gamma_y(x) alpha cdot b_mathcalY  value(p::MOMDPAlphaVectorPolicy, b, x) is provided to evaluate a MOMDPAlphaVectorPolicy with a known visible state x.","category":"page"},{"location":"policies/","page":"Policies","title":"Policies","text":"However, to maintain compatibility with simulation tools already existing within the POMDPs.jl ecosystem we also provide the ability to execute a computed MOMDP policy as an MOMDPAlphaVectorPolicy assuming a POMDP model (allowing for uncertainty over mathcalX as well). We first calculate b_mathcalX(x) = sum_y in mathcalY b(xy) and then  V^prime(b) = sum_x in mathcalX b_mathcalX(x) V(x b_mathcalY mid x) where b_mathcalY mid x = b(xy)  b_mathcalX(x). This value calcualtion is provided by value(p::MOMDPAlphaVectorPolicy, b).","category":"page"},{"location":"policies/","page":"Policies","title":"Policies","text":"When the visible state is known, we also provide actionvalues(p::MOMDPAlphaVectorPolicy, b, x) to compute the action values (Q-values) for all actions given a belief b over hidden states and a known visible state x. This performes a one-step lookahead to compute action values.","category":"page"},{"location":"policies/#Action-Function","page":"Policies","title":"Action Function","text":"","category":"section"},{"location":"policies/","page":"Policies","title":"Policies","text":"The action function action(p::MOMDPAlphaVectorPolicy, b) implements a heuristic instead of a true one step lookahead when there is uncertainty over mathcalX. If executing the MOMDP policy as a POMDP and x is not known, then we recommend implementing a custom action function that performs a one-step lookahead using the value function. ","category":"page"},{"location":"policies/","page":"Policies","title":"Policies","text":"As implemented, action finds the state x with the largest probability mass in b, forms a conditional distribution over y given that x, finds the alpha vector within the subset associated with x that maximizes the value given the conditional distirbution, and returns the actions associated with that alpha vector.","category":"page"},{"location":"policies/","page":"Policies","title":"Policies","text":"We also provide a function action(p::MOMDPAlphaVectorPolicy, b, x) that assumes the visible state x is known and avoides the need to infer x from the belief distribution.","category":"page"}]
}
